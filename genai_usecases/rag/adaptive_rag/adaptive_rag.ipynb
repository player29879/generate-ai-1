{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive-RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "It is an advanced technique that improves upon traditional RAG systems by dynamically adapting the retrieval process based on the specific query and context. This approach aims to enhance the relevance and accuracy of retrieved information, leading to better-generated responses.\n",
        "\n",
        "## Key features of Adaptive-RAG include:\n",
        "\n",
        "1. Dynamic query reformulation\n",
        "2. Iterative retrieval\n",
        "3. Adaptive chunk selection\n",
        "4. Relevance feedback"
      ],
      "metadata": {
        "id": "HxxOWz9dpsYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.2.11 \\\n",
        "     langchain-google-genai==1.0.7 \\\n",
        "     langchain-chroma==0.1.2 \\\n",
        "     langchain-community==0.2.10 \\\n",
        "     einops==0.8.0"
      ],
      "metadata": {
        "id": "ShxTNxM5gqtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import related libraries related to Langchain, HuggingfaceEmbedding"
      ],
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor"
      ],
      "metadata": {
        "id": "RL-3LsYogoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "GT55z5AkhyOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Google API Key. You can create Google API key at following lin\n",
        "\n",
        "[Google Gemini-Pro API Creation Link](https://console.cloud.google.com/apis/credentials)\n",
        "\n",
        "[YouTube Video](https://www.youtube.com/watch?v=ZHX7zxvDfoc)\n",
        "\n"
      ],
      "metadata": {
        "id": "F6UeDlrgqI2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "d55a695d-bbee-45e1-e086-7f45763032ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Huggingface API Key. You can create Huggingface API key at following lin\n",
        "\n",
        "[Higgingface API Creation Link](https://huggingface.co/settings/tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1dLpYboqeIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6scBGZlhpG",
        "outputId": "e73af7eb-bb5f-4174-87fe-9bab2b38856c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The main flow of the system is:\n",
        "\n",
        "1. Load documents into the vector store.\n",
        "2. When a query is received, it retrieves relevant documents.\n",
        "3. It then generates an answer based on these documents.\n",
        "4. If the answer isn't satisfactory, it reformulates the query and tries again.\n",
        "5. This process repeats until a satisfactory answer is found or the maximum number of iterations is reached."
      ],
      "metadata": {
        "id": "oaB23jlCpQhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveRAG:\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-pro\"):\n",
        "        \"\"\"\n",
        "        Initializes the AdaptiveRAG system.\n",
        "\n",
        "        This method:\n",
        "        - Configures Google Generative AI with the specified API key.\n",
        "        - Sets up the language model (LLM) using Google's Gemini Pro.\n",
        "        - Initializes embeddings with the nomic.ai model.\n",
        "        - Prepares placeholders for the vector store and retriever.\n",
        "        \"\"\"\n",
        "        self.llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.3, safety_settings={\n",
        "          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        },)\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})\n",
        "        self.vectorstore = None\n",
        "        self.retriever = None\n",
        "\n",
        "    def load_documents(self, documents: List[str]):\n",
        "        \"\"\"\n",
        "        Processes and loads documents into the system.\n",
        "\n",
        "        This method:\n",
        "        - Splits documents into smaller chunks for improved processing.\n",
        "        - Creates a Chroma vector store from these chunks.\n",
        "        - Sets up a retriever to fetch relevant documents.\n",
        "        \"\"\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "        )\n",
        "        texts = text_splitter.create_documents(documents)\n",
        "        self.vectorstore = Chroma.from_documents(texts, self.embeddings)\n",
        "\n",
        "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    def query_reformulation(self, query: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Creates a more targeted query from the original query and current context.\n",
        "\n",
        "        This method:\n",
        "        - Uses the LLM to generate a reformulated query based on the provided prompt.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Given the original query and the current context, please reformulate the query to be more specific and targeted:\n",
        "\n",
        "        Original query: {query}\n",
        "        Current context: {context}\n",
        "\n",
        "        Reformulated query:\"\"\"\n",
        "\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "    def generate_answer(self, query: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Generates an answer to the query based on the given context.\n",
        "\n",
        "        This method:\n",
        "        - Uses the LLM to produce an answer using the provided prompt.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Please answer the following query based on the given context:\n",
        "\n",
        "        Query: {query}\n",
        "        Context: {context}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "    def run(self, query: str, max_iterations: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Runs the adaptive RAG process.\n",
        "\n",
        "        This method iterates up to max_iterations times, performing the following steps each time:\n",
        "        - Reformulates the query (after the first iteration).\n",
        "        - Retrieves relevant documents.\n",
        "        - Generates an answer.\n",
        "        - Checks if the answer is satisfactory.\n",
        "\n",
        "        It keeps track of all iterations and returns the final answer along with iteration details.\n",
        "        \"\"\"\n",
        "        original_query = query\n",
        "        context = \"\"\n",
        "        iteration_results = []\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            if i > 0:\n",
        "                query = self.query_reformulation(original_query, context)\n",
        "\n",
        "            docs = self.retriever.get_relevant_documents(query)\n",
        "            context = \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "            answer = self.generate_answer(query, context)\n",
        "\n",
        "            iteration_results.append({\n",
        "                \"iteration\": i + 1,\n",
        "                \"query\": query,\n",
        "                \"answer\": answer,\n",
        "                \"source_documents\": docs\n",
        "            })\n",
        "\n",
        "            if self.is_answer_satisfactory(answer):\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            \"final_answer\": answer,\n",
        "            \"iterations\": iteration_results\n",
        "        }\n",
        "\n",
        "    def is_answer_satisfactory(self, answer: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if an answer is satisfactory.\n",
        "\n",
        "        This method:\n",
        "        - Evaluates whether the answer is longer than 100 characters in this simple implementation.\n",
        "        \"\"\"\n",
        "        return len(answer) > 100  # Simple length-based heuristic"
      ],
      "metadata": {
        "id": "azSUjUBugUz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This example demonstrates how to use the AdaptiveRAG system:\n",
        "\n",
        "1. Set it up with an API key\n",
        "2. Load some documents\n",
        "3. Ask a question\n",
        "4. Get and display the results, including the process the system went through to arrive at the final answer\n",
        "\n",
        "It's a simple demonstration, but it shows the core functionality of the adaptive RAG system, including its ability to potentially reformulate queries and generate answers over multiple iterations if needed."
      ],
      "metadata": {
        "id": "1CjOT95Ito5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage Example\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "adaptive_rag = AdaptiveRAG(api_key)\n",
        "\n",
        "documents = [\n",
        "    \"Paris is the capital of France.\",\n",
        "    \"London is the capital of the United Kingdom.\",\n",
        "    \"Berlin is the capital of Germany.\"\n",
        "]\n",
        "adaptive_rag.load_documents(documents)\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "result = adaptive_rag.run(query)\n",
        "\n",
        "print(f\"Final answer: {result['final_answer']}\")\n",
        "print(f\"Number of iterations: {len(result['iterations'])}\")\n",
        "for i, iteration in enumerate(result['iterations']):\n",
        "    print(f\"\\nIteration {i + 1}:\")\n",
        "    print(f\"Query: {iteration['query']}\")\n",
        "    print(f\"Answer: {iteration['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhIQtPVnherD",
        "outputId": "a9d736db-5697-40b6-cee3-3e238718dce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.e55a7d4324f65581af5f483e830b80f34680e8ff.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final answer: Paris\n",
            "Number of iterations: 3\n",
            "\n",
            "Iteration 1:\n",
            "Query: What is the capital of France?\n",
            "Answer: Paris\n",
            "\n",
            "Iteration 2:\n",
            "Query: What is the capital of France, and what is the capital of Germany?\n",
            "Answer: The capital of France is Paris, and the capital of Germany is Berlin.\n",
            "\n",
            "Iteration 3:\n",
            "Query: What is the capital of France, excluding Berlin?\n",
            "Answer: Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here are a few things to keep in mind as you continue working with this implementation:\n",
        "\n",
        "1. Performance: As you use it with larger document sets or more complex queries, pay attention to its performance and accuracy. You may need to adjust parameters like the number of retrieved documents or the maximum number of iterations.\n",
        "2. Answer Quality: The current implementation uses a simple length-based heuristic to determine if an answer is satisfactory. You might want to develop more sophisticated criteria based on your specific use case.\n",
        "3. Customization: Depending on your needs, you could expand on this foundation by adding features like logging, error handling, or integration with other data sources.\n",
        "4. Model Updates: Keep an eye on updates to the Gemini Pro model or use any other model as per your wish. As Google enhances its capabilities, you may be able to reintroduce some of the more advanced features we had to simplify.\n",
        "5. Embedding Model: The current implementation uses the nomic.ai embedding model. Depending on your specific use case, you might want to experiment with other embedding models to see if they provide better results for your particular domain."
      ],
      "metadata": {
        "id": "2SWcsM4ZtwJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6crXAkVltwyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}