{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHQaMxXHJlBD"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from typing import List\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# >>> Device: cuda\n",
        "# >>> Tesla T4"
      ],
      "metadata": {
        "id": "tYB9unhcKZnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Need to create huggingface token at following site and provide that when asked here\n",
        "\n",
        "https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "IBeb7z5tTbI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "8t1U9YsBTRrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "                                load_in_4bit=True,\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                               )\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(orig_model_path)"
      ],
      "metadata": {
        "id": "pB86JDv3JOU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "MpPNufTqJ48O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is Mistral? Write a short answer.\"\n",
        "response = mistral_llm.invoke(text)"
      ],
      "metadata": {
        "id": "aoqBV5wiQQ8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "ijbi_JHNTA2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt.format(adjective=\"funny\", content=\"chickens\")\n",
        "\n",
        "llm_chain = prompt | mistral_llm\n",
        "response = llm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})"
      ],
      "metadata": {
        "id": "GAoYki4-QZON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "dRnJdMjiS371"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}